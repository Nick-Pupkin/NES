{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d134a1-540d-431b-a1df-d665a522af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2, norm # for tests\n",
    "\n",
    "import matplotlib.pyplot as plt # for graphs\n",
    "import numpy as np # for matrix operations\n",
    "import pandas as pd # for DataFrames\n",
    "import statsmodels.api as sm # for statistics\n",
    "import time # for getting the time of the cell execution\n",
    "import scipy.stats as stats # for statistical distributions\n",
    "from scipy.stats import chi2 # to get chi-squared distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79b2e7-36d0-4068-8dce-51a5b4671fde",
   "metadata": {},
   "source": [
    "## Problem 2 \"Exchange rates again\"\n",
    "\n",
    "Recall the problem \"Testing for forecast unbiasedness\" from Assignment 3. For both types of forwards, test the conditional unbiasedness hypothesis by using the overlapping block bootstrap with the block length equal 15. With 3-month forwards, use the Newey- West HAC long run variance estimator. Compare the critical values you obtained using the asymptotic and bootstrap approaches. In case of large discrepancies, speculate on their possible sources.\n",
    "\n",
    "Task 2, HA 3:{\n",
    "\n",
    "The data file FwdSpot1.dat contains monthly spot and 1-month forward exchange rates, the datafilee FwdSpot3.dat- monthly spot and 3-month forward exchange rates, in $/foreign currency, for the British Pound, French Franc and Japanese Yen, for 1973:3 to 1992:8 (234 observations). Each row contains the month, the year, the spot rates for Pound, Franc, and Yen, and then the forward rates for the same three currencies. Download the data, then take logarithms of the rates.ces.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb6d8a-0d69-4872-a738-7ae3307a14c9",
   "metadata": {},
   "source": [
    "We are interested in testing the conditional unbiasedness hypothesis that:\n",
    "\n",
    "$E_t[st+k] = f_{t,k}$\n",
    "\n",
    "where:\n",
    "- $s_t$ is the spot rate at time t\n",
    "- $f_{t,k}$ is the forward rate for k-month forwards at time t\n",
    "- $E_t$ denotes the mathematical expectation conditional on time t information\n",
    "\n",
    "The statement above says that the forward rate is a conditionally unbiased predictor of the future spot exchange rate.\n",
    "\n",
    "To test this theory, one nests (1) within the following econometric model:\n",
    "\n",
    "$s_{t+k} - s_t = β + γ (f_{t,k} - s_t) + ε_{t+k}$ \n",
    "\n",
    "$E_t[ε_{t+k}] = 0$\n",
    "\n",
    "and test H0: β = 0; γ = 1.\n",
    "\n",
    "The current spot rate is subtracted to achieve stationarity.\n",
    "The difference $s_{t+k} - s_t$ is called the exchange rate depreciation, the difference $f_{t,k} - s_t$ the\n",
    "forward premium. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf832f-6840-44c1-b46c-e210e1860b9d",
   "metadata": {},
   "source": [
    "For the three currencies and both types of forwards, estimate (2) by OLS\n",
    "and test for conditional unbiasedness. Do not forget HAC variance estimation whenever\n",
    "appropriate; explain why it is needed or not needed. Discuss the test results.}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa7849-1c0f-4106-90b7-fe2a5d35aefb",
   "metadata": {},
   "source": [
    "### OLS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe46f4-65cd-4f77-812d-eef7f7a329fc",
   "metadata": {},
   "source": [
    "So, our model for the OLS is the following:\n",
    "\n",
    "$ed_{t+k} = \\beta + \\gamma fp_{t+k} + \\epsilon_{t+k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febdb894-f9cb-4fd9-9ddb-f6b0d01a71fd",
   "metadata": {},
   "source": [
    "### Beta:\n",
    "\n",
    "$\\hat{\\beta} = (X' X)^{-1} X' Y$\n",
    "\n",
    "$\\hat{\\beta}$ is the vector of estimated coefficients.\n",
    "\n",
    "$X$ is the matrix of predictor variables (also known as the design matrix).\n",
    "\n",
    "$Y$ is the vector of the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9fa6d-ff77-4202-956e-a53cee8c8797",
   "metadata": {},
   "source": [
    "### Covariance matrix (White)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c5201-3b4e-45d6-939e-abe73b12599a",
   "metadata": {},
   "source": [
    "To get estimated covariance matrix (White estimator):\n",
    "\n",
    "$\\hat{e} = \\hat{Y} - X * \\hat{\\beta}'$\n",
    "\n",
    "$\\hat{V}_{\\beta} = \\hat{Q}_{xx}^{-1}\\hat{V}_{xe}\\hat{Q}_{xx}^{-1}$; This is our covariance matrix.\n",
    "\n",
    "$\\hat{V}_{xe} = \\frac{1}{n}X' \\Omega X$, where $\\Omega$ is a diagonal matrix with error term $\\hat{e_{i}^{2}}$ on the $i_{}^{th}$ place.\n",
    "\n",
    "$\\hat{Q}_{xx}^{-1} = (\\frac{1}{n}X'X)_{}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bb19b-36ed-4ff8-9a67-e4e504318443",
   "metadata": {},
   "source": [
    "### Covariance matrix (HAC: Newly-West)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa86c4-4080-4676-a295-188422ba802c",
   "metadata": {},
   "source": [
    "To get estimated covariance matrix (HAC: Newly-West):\n",
    "\n",
    "$Z = X\\hat{e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f19bb-e282-4b2f-a413-34ba7d971ea2",
   "metadata": {},
   "source": [
    "lags: i = 1, ..., T-1\n",
    "\n",
    "$\\hat{\\Gamma}_j = \\frac{1}{T} \\sum_{t = max(1,1+j)}^{t = min(1,1+j)} (Z_t - \\overline{Z}_t)(Z_{t-j} - \\overline{Z}_t)'\\xrightarrow{p} \\Gamma_j$\n",
    "\n",
    "then the estimator itself:\n",
    "\n",
    "$\\hat{V}_z^{NW} = \\sum_{j=-m}^{m}(1-\\frac{|j|}{m+1})\\hat{\\Gamma}_j$\n",
    "\n",
    "where\n",
    "\n",
    "m = $\\lfloor 4(\\frac{T}{100}_{}^{1/3})  \\rfloor$\n",
    "\n",
    "$\\lfloor$ - integer part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9aa945-0890-4901-bde8-3979d4fe0aa9",
   "metadata": {},
   "source": [
    "Get the standard deviation:\n",
    "\n",
    "$se(\\hat{\\beta_j}) = \\sqrt{\\frac{1}{n}[\\hat{V_{\\beta}}]_{jj}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817d4747-57fa-4574-a7ba-fc8410d4bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS, White estimator, Newly-West estimator are implemented in the following module\n",
    "import sys\n",
    "\n",
    "path = 'C:/Users/Popov/Documents/NES_studies/Python/NES_Helper' # Местоположение файла на диске\n",
    "sys.path.append(path)\n",
    "# this module was expended in the 3 HA\n",
    "from NES_helper import Estimators as est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc2980-5c7a-4d31-8df9-fa946b3ed1b6",
   "metadata": {},
   "source": [
    "### Wald test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6056f-d9f9-4ef0-8372-92f80a671d60",
   "metadata": {},
   "source": [
    "$t_w = nh(\\hat{\\beta})'(\\hat{H}\\hat{V}_{\\beta}\\hat{H}')_{}^{-1}h(\\hat{\\beta})$\n",
    "\n",
    "$ h = H \\beta - q$, q- restriction RHS\n",
    "\n",
    "Note that \n",
    "\n",
    "$ed_{t+k} = \\beta + \\gamma fp_{t+k} + \\epsilon_{t+k}$\n",
    "\n",
    "$H_0$ : β = 0; γ = 1.\n",
    "\n",
    "Then H = [[0, 1], [1, 0]] - both hypothesis ([0, 1] - γ, [1, 0] - \\beta). Thus h = [1,0] (γ = 1, β = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfb0a37-78f6-4ff4-a064-a92df20ccef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wald_test(beta_hat, V_hat, H, q):\n",
    "    n = X.shape[0]\n",
    "    h = H @ beta_hat - q\n",
    "    W = n * h.T @ np.linalg.inv(H @ V_hat @ H.T) @ h\n",
    "    # pv2 = chi2.sf(W, H.shape[0])\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccb889-bc49-4340-9b95-16716325b37a",
   "metadata": {},
   "source": [
    "### Bootstrap Wald statistic:\n",
    "\n",
    "$$h = H \\beta - q = \\hat{\\beta}_b^{*} - \\hat{\\beta}, $$ then \n",
    "\n",
    "$$ \\{W_b^{*} = n(\\hat{\\beta}_b^{*} - \\hat{\\beta})'(\\hat{V}_{\\beta b}^{*})_{}^{-1}(\\hat{\\beta}_b^{*} - \\hat{\\beta})\\}_{b=1}^{B}$$\n",
    "\n",
    "$\\hat{\\beta}_b^{*}$ - bootstrap beta estimate (in the code - Beta_w)\n",
    "\n",
    "$\\hat{V}_{\\beta b}^{*}$ - bootstrap covarriance matrix estimate.\n",
    "\n",
    "$\\hat{\\beta}$ - OLS beta estimate (in the code - Beta0)\n",
    "\n",
    "Notation: B = N (number of bootstrap iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4dc311-1094-4098-83b8-08de0a11b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap_Wald(X, Y, N, l):\n",
    "    '''\n",
    "    X - regressors' matrix;\n",
    "    Y - dependent variable's bector;\n",
    "    N - number of bootstrap repetitions;\n",
    "    l - block size\n",
    "    '''\n",
    "    n=X.shape[0] # size of the sample\n",
    "    T = n // l * l #largest multiple of l that does not exceed n\n",
    "    Beta0 = np.linalg.inv(X.T @ X / n) @ (X.T @ Y / n) # those are asymptotic estimates (OLS) on the \n",
    "    # initial sample\n",
    "    Wb = np.zeros(N)\n",
    "\n",
    "    # Precompute all possible blocks with len l\n",
    "    all_blocks_X = [X[i:i + l] for i in range(T - l + 1)]\n",
    "    all_blocks_Y = [Y[i:i + l] for i in range(T - l + 1)]\n",
    "\n",
    "    for i in range(N): # initialize bootstrap loop\n",
    "        # Randomly sample blocks with replacement\n",
    "        sampled_indices = np.random.randint(0, len(all_blocks_X), size=T // l)\n",
    "        # 0 - starting index for sampling\n",
    "        # len(all_blocks_X) -  total number of possible blocks\n",
    "        # size=T // l - number of non-overlapping blocks of size l that fit within T\n",
    "        #(number of random integers to generate). Not really determined-\n",
    "        # could draw another number of blocks on each iteration.\n",
    "        Xsamp = np.vstack([all_blocks_X[j] for j in sampled_indices])\n",
    "        Ysamp = np.concatenate([all_blocks_Y[j] for j in sampled_indices])\n",
    "        # Xsamp: Stacks blocks of X data vertically to create a larger\n",
    "        # matrix where each row represents a data point in the bootstrap sample.\n",
    "        #Ysamp: Concatenates blocks of Y data into a single larger array,\n",
    "        # maintaining the 1-dimensional structure as Y is already a vector.\n",
    "        Wb[i] = Wald_b(Xsamp, Ysamp, Beta0)\n",
    "    return np.quantile(Wb, 0.95)\n",
    "\n",
    "def Wald_b(Xsamp, Ysamp, Beta0):\n",
    "  # Compute Estimates\n",
    "    n=Xsamp.shape[0]\n",
    "    QxxInv = np.linalg.inv(Xsamp.T @ Xsamp / n)\n",
    "    Beta_w = QxxInv @ (Xsamp.T @ Ysamp / n) \n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    ErHat2 = (Ysamp - Xsamp @ Beta_w)**2 \n",
    "    Omega = np.diag(ErHat2.reshape(n))\n",
    "    Vxe = (Xsamp.T @ Omega @ Xsamp)/ n\n",
    "    Vb = QxxInv @ Vxe @ QxxInv\n",
    "    \n",
    "    # Compute Bootstrap Wald statistics\n",
    "    h = Beta_w - Beta0 # our new restriction: Beta_w- beta estimate of OLS on Bootstrap generated population,\n",
    "    # Beta0 - beta estimate of OLS on the initial sample\n",
    "    H = np.array([[1, 0], [0, 1]])\n",
    "    W = n * h.T @ np.linalg.inv(H @ Vb @ H.T) @ h\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b39ec0-92c1-4974-aa5a-c9da9f2a6ec4",
   "metadata": {},
   "source": [
    "### k = 1 (lags) - One month prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247afd81-25d1-4db5-b581-4bbe8ea555e2",
   "metadata": {},
   "source": [
    "Econometric model:\n",
    "\n",
    "$s_{t+k} - s_t = β + γ (f_{t,k} - s_t) + ε_{t+k}$\n",
    "\n",
    "Note that as we predit only for one period, there is no oberlapping of predictions => no autocorrelations => use simple White estimator for variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756e95ae-130d-4269-9a91-4601d7cd076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>2.4755</td>\n",
       "      <td>0.2203</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>2.469621</td>\n",
       "      <td>0.220649</td>\n",
       "      <td>0.003780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>2.4869</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>2.482403</td>\n",
       "      <td>0.218873</td>\n",
       "      <td>0.003766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5720</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>2.568699</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>0.003803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5825</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>2.578110</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.003834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5072</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>2.501225</td>\n",
       "      <td>0.242823</td>\n",
       "      <td>0.003823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>1.7793</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>1.769202</td>\n",
       "      <td>0.179159</td>\n",
       "      <td>0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.8315</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>1.822495</td>\n",
       "      <td>0.184383</td>\n",
       "      <td>0.007829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9110</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>1.900904</td>\n",
       "      <td>0.195526</td>\n",
       "      <td>0.008013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9190</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>1.908094</td>\n",
       "      <td>0.199241</td>\n",
       "      <td>0.007847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9830</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>1.971895</td>\n",
       "      <td>0.207418</td>\n",
       "      <td>0.008098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1       2       3         4         5         6         7\n",
       "0    3  73  2.4755  0.2203  0.003752  2.469621  0.220649  0.003780\n",
       "1    4  73  2.4869  0.2187  0.003763  2.482403  0.218873  0.003766\n",
       "2    5  73  2.5720  0.2305  0.003788  2.568699  0.230600  0.003803\n",
       "3    6  73  2.5825  0.2410  0.003805  2.578110  0.241000  0.003834\n",
       "4    7  73  2.5072  0.2424  0.003772  2.501225  0.242823  0.003823\n",
       "..  ..  ..     ...     ...       ...       ...       ...       ...\n",
       "229  4  92  1.7793  0.1801  0.007513  1.769202  0.179159  0.007508\n",
       "230  5  92  1.8315  0.1853  0.007834  1.822495  0.184383  0.007829\n",
       "231  6  92  1.9110  0.1966  0.008019  1.900904  0.195526  0.008013\n",
       "232  7  92  1.9190  0.2004  0.007852  1.908094  0.199241  0.007847\n",
       "233  8  92  1.9830  0.2086  0.008103  1.971895  0.207418  0.008098\n",
       "\n",
       "[234 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('FwdSpot1.dat', header=None,\n",
    "                    sep=' ', skipinitialspace=True)\n",
    "# skipinitialspace=True - to exclude the leading spaces - because otherwise\n",
    "# we a column of nans and a colun of dates\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee212-b7f6-45f8-974e-a67a91038ef0",
   "metadata": {},
   "source": [
    "Note that date is in the descending order. (column 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808791be-754a-459f-aa1a-de0059b4aafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BP</th>\n",
       "      <th>FF</th>\n",
       "      <th>JY</th>\n",
       "      <th>BP_f</th>\n",
       "      <th>FF_f</th>\n",
       "      <th>JY_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.906442</td>\n",
       "      <td>-1.512765</td>\n",
       "      <td>-5.585466</td>\n",
       "      <td>0.904065</td>\n",
       "      <td>-1.511182</td>\n",
       "      <td>-5.578031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.911037</td>\n",
       "      <td>-1.520054</td>\n",
       "      <td>-5.582539</td>\n",
       "      <td>0.909227</td>\n",
       "      <td>-1.519264</td>\n",
       "      <td>-5.581742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944684</td>\n",
       "      <td>-1.467504</td>\n",
       "      <td>-5.575917</td>\n",
       "      <td>0.943400</td>\n",
       "      <td>-1.467071</td>\n",
       "      <td>-5.571965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BP        FF        JY      BP_f      FF_f      JY_f\n",
       "0  0.906442 -1.512765 -5.585466  0.904065 -1.511182 -5.578031\n",
       "1  0.911037 -1.520054 -5.582539  0.909227 -1.519264 -5.581742\n",
       "2  0.944684 -1.467504 -5.575917  0.943400 -1.467071 -5.571965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the data\n",
    "data = data.iloc[:, 2:]\n",
    "# rename columns according to the task\n",
    "data.rename(columns={2: 'BP', 3: 'FF', 4: 'JY', 5: 'BP_f', 6: 'FF_f', 7: 'JY_f'}, inplace=True)\n",
    "# take logarithms\n",
    "data = data.apply(np.log)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7533168-aa3d-4959-8dc1-68d290b5cc92",
   "metadata": {},
   "source": [
    "For our coefficients estimate we will use standard OLS (as they ask us in the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a9296c-992b-4a3e-b5fc-3069970c9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's at first do only for the 1-lag\n",
    "k = 1\n",
    "# note, that in our dataset previous years are at the bottom\n",
    "# lagged spot prices for pound, franc and yen.\n",
    "\n",
    "s_t_k = data.iloc[:, :3]\n",
    "\n",
    "# current values\n",
    "s_t = s_t_k.shift(-k)\n",
    "# calculate the exchange rate depreciation\n",
    "e_d = (s_t - s_t_k.values).dropna().values\n",
    "# current futures values\n",
    "f_t_k = data.iloc[:, 3:]\n",
    "# now calculate the forward premium\n",
    "f_p = (f_t_k - s_t_k.values).iloc[:e_d.shape[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc15255-3961-4f12-8530-df4ad4df61b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS for BP\n",
      "alpha_1: -0.0023 (0.0024)\n",
      "beta_1: -0.7261 (0.6401)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 7.4459\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 13.6796\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "OLS for FF\n",
      "alpha_2: -0.0023 (0.0026)\n",
      "beta_2: -0.9606 (0.85)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 5.7265\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 9.6387\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "OLS for JY\n",
      "alpha_3: 0.0036 (0.0023)\n",
      "beta_3: -0.1528 (0.5595)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 4.8988\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 15.9921\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "CPU times: total: 1.2 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coef_names = [['alpha_1', 'beta_1'], ['alpha_2', 'beta_2'], ['alpha_3', 'beta_3']]\n",
    "names = data.columns.tolist() \n",
    "\n",
    "# Loop through OLS (for each currency)\n",
    "for i, name in enumerate(coef_names):\n",
    "    print(\"OLS for \" + names[i])\n",
    "    \n",
    "    X = sm.add_constant(f_p[:, i])\n",
    "    Y = e_d[:, i]\n",
    "    \n",
    "    # Initialize the Estimators class with data\n",
    "    esti = est(X, Y)\n",
    "    \n",
    "    # Estimate beta\n",
    "    beta_hat = esti.beta_est()\n",
    "    \n",
    "    # Calculate White standard errors\n",
    "    V_hat = esti.White_est(beta_hat)\n",
    "    SDs = esti.SD(V_hat)\n",
    "    \n",
    "    # Perform Wald test\n",
    "    w = Wald_test(beta_hat, V_hat, np.array([[0, 1], [1, 0]]), np.array([1, 0]))\n",
    "    df = np.array([[0, 1], [1, 0]]).shape[0] # number of restrictions is te degress of freedom for\n",
    "    # chi-squared to which Wald statistic is converging\n",
    "    \n",
    "    # Perform bootstrap Wald test\n",
    "    \n",
    "    N = 100000 # number of repetitions\n",
    "    l = 15 # block size\n",
    "    wb = Bootstrap_Wald(X, Y, N, l)\n",
    "\n",
    "    # Print coefficients with their standard errors\n",
    "    for coef, coef_name, sd in zip(beta_hat, coef_names[i], SDs):\n",
    "        print(f\"{coef_name}: {round(coef, 4)} ({round(sd, 4)})\")\n",
    "    print('----  ----  ----  ----  ----')   \n",
    "    print(\"Wald statistic\" + \"is:\", round(w, 4), end=\"\\n\\n\")\n",
    "    print(\"Critical value (95% quantile of chi-squared) for Wald statistic\", round(stats.chi2.ppf(0.95, df),4))\n",
    "    print(\"Bootstrap Wald statistic 95% quantile (simulated critical value) \" + \"is:\", round(wb, 4))\n",
    "    print('--------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9228f8-cac3-4aa5-8dcc-30396cd2e5a5",
   "metadata": {},
   "source": [
    "On 5% level the  is not rejected for franc, yen, but is rejected for pound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bf9be-2862-4901-8283-d346f3c228b6",
   "metadata": {},
   "source": [
    "### k = 3 (lags) - Three month prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aaa3fa7-c614-4288-8ce0-126c0bac0665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>2.4755</td>\n",
       "      <td>0.2203</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>2.458357</td>\n",
       "      <td>0.221151</td>\n",
       "      <td>0.003840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>2.4869</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>2.473906</td>\n",
       "      <td>0.219171</td>\n",
       "      <td>0.003782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5720</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>2.562098</td>\n",
       "      <td>0.230973</td>\n",
       "      <td>0.003843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5825</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>2.571008</td>\n",
       "      <td>0.241259</td>\n",
       "      <td>0.003886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "      <td>2.5072</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>2.488521</td>\n",
       "      <td>0.243514</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>1.7793</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>1.751900</td>\n",
       "      <td>0.177368</td>\n",
       "      <td>0.007502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.8315</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>1.804800</td>\n",
       "      <td>0.182498</td>\n",
       "      <td>0.007821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9110</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>1.882100</td>\n",
       "      <td>0.193424</td>\n",
       "      <td>0.008004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9190</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>1.886600</td>\n",
       "      <td>0.196951</td>\n",
       "      <td>0.007840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>1.9830</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>1.948900</td>\n",
       "      <td>0.204918</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1       2       3         4         5         6         7\n",
       "0    3  73  2.4755  0.2203  0.003752  2.458357  0.221151  0.003840\n",
       "1    4  73  2.4869  0.2187  0.003763  2.473906  0.219171  0.003782\n",
       "2    5  73  2.5720  0.2305  0.003788  2.562098  0.230973  0.003843\n",
       "3    6  73  2.5825  0.2410  0.003805  2.571008  0.241259  0.003886\n",
       "4    7  73  2.5072  0.2424  0.003772  2.488521  0.243514  0.003919\n",
       "..  ..  ..     ...     ...       ...       ...       ...       ...\n",
       "229  4  92  1.7793  0.1801  0.007513  1.751900  0.177368  0.007502\n",
       "230  5  92  1.8315  0.1853  0.007834  1.804800  0.182498  0.007821\n",
       "231  6  92  1.9110  0.1966  0.008019  1.882100  0.193424  0.008004\n",
       "232  7  92  1.9190  0.2004  0.007852  1.886600  0.196951  0.007840\n",
       "233  8  92  1.9830  0.2086  0.008103  1.948900  0.204918  0.008095\n",
       "\n",
       "[234 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('FwdSpot3.dat', header=None,\n",
    "                    sep=' ', skipinitialspace=True)\n",
    "# skipinitialspace=True - to exclude the leading spaces - becuase otherwise\n",
    "# we a column of nans\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ea4e93-a490-44ab-a056-679abbc73619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BP</th>\n",
       "      <th>FF</th>\n",
       "      <th>JY</th>\n",
       "      <th>BP_f</th>\n",
       "      <th>FF_f</th>\n",
       "      <th>JY_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.906442</td>\n",
       "      <td>-1.512765</td>\n",
       "      <td>-5.585466</td>\n",
       "      <td>0.899493</td>\n",
       "      <td>-1.508910</td>\n",
       "      <td>-5.562283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.911037</td>\n",
       "      <td>-1.520054</td>\n",
       "      <td>-5.582539</td>\n",
       "      <td>0.905798</td>\n",
       "      <td>-1.517903</td>\n",
       "      <td>-5.577502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944684</td>\n",
       "      <td>-1.467504</td>\n",
       "      <td>-5.575917</td>\n",
       "      <td>0.940826</td>\n",
       "      <td>-1.465454</td>\n",
       "      <td>-5.561502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BP        FF        JY      BP_f      FF_f      JY_f\n",
       "0  0.906442 -1.512765 -5.585466  0.899493 -1.508910 -5.562283\n",
       "1  0.911037 -1.520054 -5.582539  0.905798 -1.517903 -5.577502\n",
       "2  0.944684 -1.467504 -5.575917  0.940826 -1.465454 -5.561502"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the data\n",
    "data = data.iloc[:, 2:]\n",
    "# rename columns according to the task\n",
    "data.rename(columns={2: 'BP', 3: 'FF', 4: 'JY', 5: 'BP_f', 6: 'FF_f', 7: 'JY_f'}, inplace=True)\n",
    "# take logarithms\n",
    "data = data.apply(np.log)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d9bce7-dc03-4c97-b6e9-06c29656c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's at first do only for the 1-lag\n",
    "k = 3\n",
    "# note, that in order to construct our model,\n",
    "# we need to say that we ate in the period t+k. Then k-lag - period t.\n",
    "# current spot prices for pound, franc and yen.\n",
    "\n",
    "s_t_k = data.iloc[:, :3]\n",
    "\n",
    "# lagged values\n",
    "s_t = s_t_k.shift(-k)\n",
    "# calculate the exchange rate depreciation\n",
    "e_d = (s_t - s_t_k.values).dropna().values\n",
    "# current futures values\n",
    "f_t_k = data.iloc[:, 3:]\n",
    "# now calculate the forward premium\n",
    "f_p = (f_t_k - s_t_k.values).iloc[:e_d.shape[0]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b4e4f-0531-430c-a692-f087af01b4c7",
   "metadata": {},
   "source": [
    "Because k = 3 > 1(# of lags) => $e_t$ serially correlated. Thus, HAC is needed => Newey-West estimator (the standard one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ac4c69-c897-4c10-80e8-6465f8f3b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS for BP\n",
      "alpha_1: -0.0187 (0.0076)\n",
      "beta_1: -2.0586 (0.7002)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 19.094\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 22.4234\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "OLS for FF\n",
      "alpha_2: -0.0044 (0.0091)\n",
      "beta_2: -0.4804 (0.8427)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 3.7572\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 28.9017\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "OLS for JY\n",
      "alpha_3: 0.0131 (0.0071)\n",
      "beta_3: -0.602 (0.4634)\n",
      "----  ----  ----  ----  ----\n",
      "Wald statisticis: 12.5791\n",
      "\n",
      "Critical value (95% quantile of chi-squared) for Wald statistic 5.9915\n",
      "Bootstrap Wald statistic 95% quantile (simulated critical value) is: 34.6868\n",
      "--------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through OLS (for each currency)\n",
    "coef_names = [['alpha_1', 'beta_1'], ['alpha_2', 'beta_2'], ['alpha_3', 'beta_3']]\n",
    "names = data.columns.tolist()\n",
    "for i, name in enumerate(coef_names):\n",
    "    print(\"OLS for \" + names[i])\n",
    "    \n",
    "    X = sm.add_constant(f_p[:, i])\n",
    "    Y = e_d[:, i]\n",
    "    \n",
    "    # Initialize the Estimators class with data\n",
    "    esti = est(X, Y)\n",
    "    \n",
    "    # Estimate beta\n",
    "    beta_hat = esti.beta_est()\n",
    "    \n",
    "    # Calculate Newey-West standard errors\n",
    "    V_hat = esti.NW_est(beta_hat)\n",
    "    SDs = esti.SD(V_hat)\n",
    "    \n",
    "    # Perform Wald test\n",
    "    w = Wald_test(beta_hat, V_hat, np.array([[0, 1], [1, 0]]), np.array([1, 0]))\n",
    "    df = np.array([[0, 1], [1, 0]]).shape[0] # number of restrictions is te degress of freedom for\n",
    "    # chi-squared to which Wald statistic is converging\n",
    "    \n",
    "    # Perform bootstrap Wald test\n",
    "    \n",
    "    N = 100000 # number of repetitions\n",
    "    l = 15 # block size\n",
    "    wb = Bootstrap_Wald(X, Y, N, l)\n",
    "\n",
    "    # Print coefficients with their standard errors\n",
    "    for coef, coef_name, sd in zip(beta_hat, coef_names[i], SDs):\n",
    "        print(f\"{coef_name}: {round(coef, 4)} ({round(sd, 4)})\")\n",
    "    print('----  ----  ----  ----  ----')   \n",
    "    print(\"Wald statistic\" + \"is:\", round(w, 4), end=\"\\n\\n\")\n",
    "    print(\"Critical value (95% quantile of chi-squared) for Wald statistic\", round(stats.chi2.ppf(0.95, df),4))\n",
    "    print(\"Bootstrap Wald statistic 95% quantile (simulated critical value) \" + \"is:\", round(wb, 4))\n",
    "    print('--------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73aabbe-fd95-4b27-a2dd-7d2aa764c567",
   "metadata": {},
   "source": [
    "$H_0$: forward rate is a conditionally unbiased predictor of the future spot exchange rate\n",
    "\n",
    "On 5% level the $H_0$ is not rejected for franc, but is rejected for pound, yen. (3 months forecast)\n",
    "\n",
    "recall the results for one month forecast:\n",
    "\n",
    "On 5% level the $H_0$ is not rejected for franc, yen, but is rejected for pound. (1 month forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d70dcd-8d20-45e9-8dca-0180c3190c1b",
   "metadata": {},
   "source": [
    "Two procedures suggest that the null hypothesis is rather not true for the pound, but true for the franc. For Yen it isn’t that obvious (for 1 month forecast- rejected $H_0$, 3 months forecast - not).\n",
    "\n",
    "Given that beta are negative and not nearly 1 as assumed, the cases when the null hypothesis is not rejected cause doubts.\n",
    "\n",
    "As for the bootstrap, it's critical values are all above critical values of chi-squared statistic. According to bootstrap- we should reject the hypothesis for all forecasts (1 and 3 months) and for all the currencies (pound, franc, yen). So, forward rate is NOT a conditionally unbiased predictor of the future spot exchange rate.\n",
    "\n",
    "We know that bootstrap with large amount of iterations has higher quality of approximation (higher convergence speed) on the finite samples than asymptotic estimates. In our case, there only 234 observations in the sample - not nearly enough for proper asymptotics. Note that all the talks about variance and the quality of approximations take the consistency of (OLS) estimates as given. If the estimates are not consistent for some reason (we misjudged the model, for example), their rate of convergence doesn't matter, because they converge to nowhere => Wald test is simply irrelevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
